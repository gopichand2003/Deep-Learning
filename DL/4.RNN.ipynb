{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17812364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
      "label: 0\n",
      "WARNING:tensorflow:From C:\\Users\\online\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\online\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "[-0.00618792]\n",
      "$$$$$$$$$$$$$$$\n",
      "Epoch 1/2\n",
      "391/391 [==============================] - 477s 1s/step - loss: 0.6365 - accuracy: 0.5714 - val_loss: 0.5316 - val_accuracy: 0.6771\n",
      "Epoch 2/2\n",
      "391/391 [==============================] - 515s 1s/step - loss: 0.4521 - accuracy: 0.7708 - val_loss: 0.4234 - val_accuracy: 0.7911\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'evaliuate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 33\u001b[0m\n\u001b[0;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mBinaryCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     28\u001b[0m              optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;241m1e-4\u001b[39m),\n\u001b[0;32m     29\u001b[0m              metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     30\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_dataset, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     31\u001b[0m                    validation_data\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[0;32m     32\u001b[0m                    validation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m test_loss,test_acc\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaliuate\u001b[49m(test_dataset)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Los:\u001b[39m\u001b[38;5;124m'\u001b[39m,test_loss)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mtest_acc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'evaliuate'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "dataset,info = tfds.load('imdb_reviews',with_info=True,as_supervised=True)\n",
    "train_dataset,test_dataset = dataset['train'],dataset['test']\n",
    "train_dataset.element_spec\n",
    "for example,label in train_dataset.take(1):\n",
    "    print('text:',example.numpy())\n",
    "    print('label:',label.numpy())\n",
    "    BUFFER_SIZE=10000\n",
    "    BATCH_SIZE=64\n",
    "    train_dataset=train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    test_dataset=test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    VOCAB_SIZE=1000\n",
    "    encoder=tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "    encoder.adapt(train_dataset.map(lambda text,label:text))\n",
    "    model=tf.keras.Sequential([encoder,tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()),output_dim=64,mask_zero=True),\n",
    "                              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "                              tf.keras.layers.Dense(64,activation='relu'),\n",
    "                              tf.keras.layers.Dense(1)\n",
    "                              ])\n",
    "    sample_text=('The movie was cool. The animation and the graphics'\n",
    "                'were out of this world. I would recommend this movie')\n",
    "    predictions = model.predict(np.array([sample_text]))\n",
    "    print(predictions[0])\n",
    "    print(\"$$$$$$$$$$$$$$$\")\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                 optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                 metrics=['accuracy'])\n",
    "    history = model.fit(train_dataset, epochs=2,\n",
    "                       validation_data=test_dataset,\n",
    "                       validation_steps=30)\n",
    "    test_loss,test_acc=model.evaluate(test_dataset)\n",
    "    print('Test Los:',test_loss)\n",
    "    print('Test Accuracy:'.test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31464a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
